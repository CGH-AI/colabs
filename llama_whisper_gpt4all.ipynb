{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhX9npPVIT7/ScxkA1s1rs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CGHAI/colabs/blob/main/llama_whisper_gpt4all.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzcCK4Xjo6Jt",
        "outputId": "378191f2-440a-44ca-edf1-a378e76c7fc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 1185, done.\u001b[K\n",
            "remote: Counting objects: 100% (1185/1185), done.\u001b[K\n",
            "remote: Compressing objects: 100% (465/465), done.\u001b[K\n",
            "remote: Total 1185 (delta 727), reused 1120 (delta 693), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1185/1185), 1.04 MiB | 13.46 MiB/s, done.\n",
            "Resolving deltas: 100% (727/727), done.\n",
            "/content/llama.cpp\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:  Linux\n",
            "I UNAME_P:  x86_64\n",
            "I UNAME_M:  x86_64\n",
            "I CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -mavx -mavx2 -mfma -mf16c -msse3\n",
            "I CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread\n",
            "I LDFLAGS:  \n",
            "I CC:       cc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "I CXX:      g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "\n",
            "cc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -mavx -mavx2 -mfma -mf16c -msse3   -c ggml.c -o ggml.o\n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread -c llama.cpp -o llama.o\n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread -c examples/common.cpp -o common.o\n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/main/main.cpp ggml.o llama.o common.o -o main \n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/quantize/quantize.cpp ggml.o llama.o -o quantize \n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/perplexity/perplexity.cpp ggml.o llama.o common.o -o perplexity \n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -pthread examples/embedding/embedding.cpp ggml.o llama.o common.o -o embedding \n"
          ]
        }
      ],
      "source": [
        "#@markdown Clone and build llama.cpp\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "%cd llama.cpp\n",
        "!make"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Obtain the original LLaMA model weights and place them in ./models<br>\n",
        "#@markdown `ls ./models`<br>\n",
        "#@markdown 65B 30B 13B 7B tokenizer_checklist.chk tokenizer.model<br>\n",
        "!ls ./models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqgg6jaiqSoO",
        "outputId": "94228b9c-7d49-4c04-cf66-d0aaea68a770"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ggml-vocab.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Clone and build whisper.cpp\n",
        "%cd ..\n",
        "!git clone https://github.com/CGHAI/whisper.cpp.git\n",
        "%cd whisper.cpp\n",
        "!make"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J59OwGg5siFX",
        "outputId": "5c539c00-aba5-4795-8cfc-d93a4cea0bee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'whisper.cpp'...\n",
            "remote: Enumerating objects: 2917, done.\u001b[K\n",
            "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
            "remote: Total 2917 (delta 40), reused 57 (delta 27), pack-reused 2832\u001b[K\n",
            "Receiving objects: 100% (2917/2917), 5.05 MiB | 20.02 MiB/s, done.\n",
            "Resolving deltas: 100% (1795/1795), done.\n",
            "/content/whisper.cpp\n",
            "I whisper.cpp build info: \n",
            "I UNAME_S:  Linux\n",
            "I UNAME_P:  x86_64\n",
            "I UNAME_M:  x86_64\n",
            "I CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx -mavx2 -mfma -mf16c -msse3\n",
            "I CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread\n",
            "I LDFLAGS:  \n",
            "I CC:       cc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "I CXX:      g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "\n",
            "cc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -pthread -mavx -mavx2 -mfma -mf16c -msse3   -c ggml.c -o ggml.o\n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread -c whisper.cpp -o whisper.o\n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread examples/main/main.cpp examples/common.cpp ggml.o whisper.o -o main \n",
            "./main -h\n",
            "\n",
            "usage: ./main [options] file0.wav file1.wav ...\n",
            "\n",
            "options:\n",
            "  -h,        --help              [default] show this help message and exit\n",
            "  -t N,      --threads N         [2      ] number of threads to use during computation\n",
            "  -p N,      --processors N      [1      ] number of processors to use during computation\n",
            "  -ot N,     --offset-t N        [0      ] time offset in milliseconds\n",
            "  -on N,     --offset-n N        [0      ] segment index offset\n",
            "  -d  N,     --duration N        [0      ] duration of audio to process in milliseconds\n",
            "  -mc N,     --max-context N     [-1     ] maximum number of text context tokens to store\n",
            "  -ml N,     --max-len N         [0      ] maximum segment length in characters\n",
            "  -sow,      --split-on-word     [false  ] split on word rather than on token\n",
            "  -bo N,     --best-of N         [5      ] number of best candidates to keep\n",
            "  -bs N,     --beam-size N       [-1     ] beam size for beam search\n",
            "  -wt N,     --word-thold N      [0.01   ] word timestamp probability threshold\n",
            "  -et N,     --entropy-thold N   [2.40   ] entropy threshold for decoder fail\n",
            "  -lpt N,    --logprob-thold N   [-1.00  ] log probability threshold for decoder fail\n",
            "  -su,       --speed-up          [false  ] speed up audio by x2 (reduced accuracy)\n",
            "  -tr,       --translate         [false  ] translate from source language to english\n",
            "  -di,       --diarize           [false  ] stereo audio diarization\n",
            "  -nf,       --no-fallback       [false  ] do not use temperature fallback while decoding\n",
            "  -otxt,     --output-txt        [false  ] output result in a text file\n",
            "  -ovtt,     --output-vtt        [false  ] output result in a vtt file\n",
            "  -osrt,     --output-srt        [false  ] output result in a srt file\n",
            "  -owts,     --output-words      [false  ] output script for generating karaoke video\n",
            "  -fp,       --font-path         [/System/Library/Fonts/Supplemental/Courier New Bold.ttf] path to a monospace font for karaoke video\n",
            "  -ocsv,     --output-csv        [false  ] output result in a CSV file\n",
            "  -oj,       --output-json       [false  ] output result in a JSON file\n",
            "  -of FNAME, --output-file FNAME [       ] output file path (without file extension)\n",
            "  -ps,       --print-special     [false  ] print special tokens\n",
            "  -pc,       --print-colors      [false  ] print colors\n",
            "  -pp,       --print-progress    [false  ] print progress\n",
            "  -nt,       --no-timestamps     [true   ] do not print timestamps\n",
            "  -l LANG,   --language LANG     [en     ] spoken language ('auto' for auto-detect)\n",
            "             --prompt PROMPT     [       ] initial prompt\n",
            "  -m FNAME,  --model FNAME       [models/ggml-base.en.bin] model path\n",
            "  -f FNAME,  --file FNAME        [       ] input WAV file path\n",
            "\n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -pthread examples/bench/bench.cpp ggml.o whisper.o -o bench \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Download ggml model\n",
        "%cd models\n",
        "!./download-ggml-model.sh base.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iG2wUXOpu5KU",
        "outputId": "938fa0fe-5cdd-4895-f2d1-5ac45cb7c7b4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/whisper.cpp/models\n",
            "Downloading ggml model base.en from 'https://huggingface.co/ggerganov/whisper.cpp' ...\n",
            "ggml-base.en.bin    100%[===================>] 141.11M   163MB/s    in 0.9s    \n",
            "Done! Model 'base.en' saved in 'models/ggml-base.en.bin'\n",
            "You can now use it like this:\n",
            "\n",
            "  $ ./main -m models/ggml-base.en.bin -f samples/jfk.wav\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Execute the example\n",
        "%cd ..\n",
        "!./main -m models/ggml-base.en.bin -f samples/jfk.wav"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opBx7-6zvNKr",
        "outputId": "e09cd97e-59c9-44dd-e924-03e7126fc698"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/whisper.cpp\n",
            "whisper_init_from_file_no_state: loading model from 'models/ggml-base.en.bin'\n",
            "whisper_model_load: loading model\n",
            "whisper_model_load: n_vocab       = 51864\n",
            "whisper_model_load: n_audio_ctx   = 1500\n",
            "whisper_model_load: n_audio_state = 512\n",
            "whisper_model_load: n_audio_head  = 8\n",
            "whisper_model_load: n_audio_layer = 6\n",
            "whisper_model_load: n_text_ctx    = 448\n",
            "whisper_model_load: n_text_state  = 512\n",
            "whisper_model_load: n_text_head   = 8\n",
            "whisper_model_load: n_text_layer  = 6\n",
            "whisper_model_load: n_mels        = 80\n",
            "whisper_model_load: f16           = 1\n",
            "whisper_model_load: type          = 2\n",
            "whisper_model_load: mem required  =  218.00 MB (+    6.00 MB per decoder)\n",
            "whisper_model_load: adding 1607 extra tokens\n",
            "whisper_model_load: model ctx     =  140.60 MB\n",
            "whisper_model_load: model size    =  140.54 MB\n",
            "whisper_init_state: kv self size  =    5.25 MB\n",
            "whisper_init_state: kv cross size =   17.58 MB\n",
            "\n",
            "system_info: n_threads = 2 / 2 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
            "\n",
            "main: processing 'samples/jfk.wav' (176000 samples, 11.0 sec), 2 threads, 1 processors, lang = en, task = transcribe, timestamps = 1 ...\n",
            "\n",
            "\n",
            "[00:00:00.000 --> 00:00:11.000]   And so my fellow Americans, ask not what your country can do for you, ask what you can do for your country.\n",
            "\n",
            "\n",
            "whisper_print_timings:     load time =   389.85 ms\n",
            "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
            "whisper_print_timings:      mel time =   337.41 ms\n",
            "whisper_print_timings:   sample time =    25.59 ms /    27 runs (    0.95 ms per run)\n",
            "whisper_print_timings:   encode time =  8196.48 ms /     1 runs ( 8196.48 ms per run)\n",
            "whisper_print_timings:   decode time =   817.19 ms /    27 runs (   30.27 ms per run)\n",
            "whisper_print_timings:    total time =  9970.45 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install Python dependencies\n",
        "!pip install torch numpy sentencepiece"
      ],
      "metadata": {
        "id": "eAxqlwOxqP9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the 7B model to ggml FP16 format\n",
        "python3 convert-pth-to-ggml.py models/7B/ 1"
      ],
      "metadata": {
        "id": "U4hFvzfpqHzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quantize the model to 4-bits\n",
        "python3 quantize.py 7B"
      ],
      "metadata": {
        "id": "9HmNadY7qJmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the inference\n",
        "./main -m ./models/7B/ggml-model-q4_0.bin -n 128"
      ],
      "metadata": {
        "id": "DYXeV3iOqLv8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}